{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43872562-b52c-42fb-a039-04edc0537e18",
   "metadata": {},
   "source": [
    "## Bert For Fake News Classification:\n",
    "### Yassin Bahid\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art natural language processing (NLP) model developed by Google. It is a pre-trained language model that uses a transformer-based neural network architecture to learn contextual relations between words in a sentence. The BERT model is trained using a large amount of text data and is designed to understand the meaning of text in a way that is closer to how humans do. Unlike previous language models, BERT can take into account the context of a word within a sentence, as well as the context of the sentence within a larger body of text. The BERT model is bidirectional, which means it can take into account both the preceding and following words in a sentence when making predictions. This allows it to have a better understanding of the context in which a word is being used.\n",
    "BERT has achieved state-of-the-art results on a wide range of NLP tasks, including sentiment analysis, named entity recognition, question answering, and more. It has been used in a variety of applications, such as chatbots, language translation, and text classification. BERT is an important advancement in the field of NLP, as it provides a powerful tool for understanding and processing natural language text in a way that is more accurate and nuanced than previous models. \n",
    "\n",
    "In this report, we modify the original BERT pretrained model to classify fake news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b9ac0-79db-40fb-9fb7-310bc14c3cf8",
   "metadata": {},
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d080cd23-9123-424a-a8bd-59bb8c609704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running On...: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Running On...:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f567bb-895f-4b95-bbed-212eac99983f",
   "metadata": {},
   "source": [
    "## 2 - Data:\n",
    "\n",
    "We use a Kaggle dataset of News articles which were determined to either be true or false. https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv. We devide this dataset into a training, validation, and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aae74a1-2ae0-4f48-928d-b274e799bcb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
       "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
       "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 15, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
       "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 14, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      As U.S. budget fight looms, Republicans flip t...   \n",
       "1      U.S. military to accept transgender recruits o...   \n",
       "2      Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3      FBI Russia probe helped by Australian diplomat...   \n",
       "4      Trump wants Postal Service to charge 'much mor...   \n",
       "...                                                  ...   \n",
       "44893  McPain: John McCain Furious That Iran Treated ...   \n",
       "44894  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
       "44895  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
       "44896  How to Blow $700 Million: Al Jazeera America F...   \n",
       "44897  10 U.S. Navy Sailors Held by Iranian Military ...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "0      WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1      WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2      WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3      WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4      SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "...                                                  ...           ...   \n",
       "44893  21st Century Wire says As 21WIRE reported earl...   Middle-east   \n",
       "44894  21st Century Wire says It s a familiar theme. ...   Middle-east   \n",
       "44895  Patrick Henningsen  21st Century WireRemember ...   Middle-east   \n",
       "44896  21st Century Wire says Al Jazeera America will...   Middle-east   \n",
       "44897  21st Century Wire says As 21WIRE predicted in ...   Middle-east   \n",
       "\n",
       "                     date  label  \n",
       "0      December 31, 2017       1  \n",
       "1      December 29, 2017       1  \n",
       "2      December 31, 2017       1  \n",
       "3      December 30, 2017       1  \n",
       "4      December 29, 2017       1  \n",
       "...                   ...    ...  \n",
       "44893    January 16, 2016      0  \n",
       "44894    January 16, 2016      0  \n",
       "44895    January 15, 2016      0  \n",
       "44896    January 14, 2016      0  \n",
       "44897    January 12, 2016      0  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truedf = pd.read_csv('data/True.csv')\n",
    "truedf['label'] = 1\n",
    "\n",
    "fakedf = pd.read_csv('data/Fake.csv')\n",
    "fakedf['label'] = 0\n",
    "\n",
    "data = pd.concat([truedf, fakedf], ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ef9b6a-a4d4-4482-ab64-5385b175eb48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "labels_idx = {'Fake':0,\n",
    "          'True':1\n",
    "          }\n",
    "\n",
    "class DataClass(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.loc[index, 'text'])\n",
    "        label = int(self.data.loc[index, 'label'])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaeb8f91-d737-4573-aec1-e81bdf7053c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initializing a training, valation, and testinng data set:\n",
    "\n",
    "traindf, valdf, testdf = np.split(data.sample(frac=1, random_state=np.random.seed(round(time.time()))), \n",
    "                                     [int(.8*len(data)), int(.9*len(data))])\n",
    "\n",
    "\n",
    "traindf, valdf, testdf = traindf.reset_index(drop=True), valdf.reset_index(drop=True), testdf.reset_index(drop=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train, val, test = DataClass(traindf,  tokenizer, max_length=256), DataClass(valdf, tokenizer, max_length=256), DataClass(testdf,  tokenizer, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0549df9-e9e8-45a3-a2b6-0665bd9fbe9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35918\n",
      "4490\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a1a440-c9f8-496c-bf9a-c03f12bf85a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455e461-d791-4ce9-ba51-f9e7c761cf57",
   "metadata": {},
   "source": [
    "## 3 - Bert Model:\n",
    "\n",
    "### 3.1 Architechture:\n",
    "BERT consists of two parts: the encoder and the decoder. The encoder is a multi-layer bidirectional transformer, which means it processes the input text in both directions (left-to-right and right-to-left) and uses self-attention to capture the relationships between words in the text. The decoder is typically used for tasks such as text classification or question answering, where the model is given a specific task to perform.\n",
    "The encoder consists of 12 or 24 transformer layers, depending on the specific version of BERT used. Each layer has two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network. The multi-head self-attention mechanism allows the model to focus on different parts of the input text at different levels of granularity, while the feed-forward neural network provides a non-linear transformation of the input.\n",
    "After passing through the final transformer layer, the output is processed by a pooling layer, which generates a fixed-size representation of the input text. This representation is subsequently used as input to the decoder, which is typically a simple feed-forward neural network that generates the final output for the given NLP task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c058c1f5-b141-4e58-99b0-d78d42cdbb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Bert_outputs  = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # pooled_output =  Bert_outputs['last_hidden_state']\n",
    "        # dropout_output = self.dropout(pooled_output)\n",
    "        # linear_output = self.linear(dropout_output)\n",
    "        # final_layer = self.relu(linear_output)\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_ids, attention_mask=attention_mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e48f3ec-0a5f-4012-a3db-3de671b6275b",
   "metadata": {},
   "source": [
    "### 3.2 - Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4cc094-5c40-4e6a-b13f-8be392e416d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   |  Train Loss: tensor(0.0002, device='cuda:0') |  Val Loss:  0.0004038849848575106  | Val Accuracy: 4490\n",
      "epoch: 1   |  Train Loss: tensor(2.7299e-05, device='cuda:0') |  Val Loss:  0.007017729406151778  | Val Accuracy: 4487\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(num_classes=2).to(device)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "num_epochs = 2\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        # print(outputs.shape)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # valate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * len(labels)\n",
    "            val_correct += sum(torch.argmax(outputs, dim=1) == labels).item()\n",
    "\n",
    "    val_loss /= len(valdf)\n",
    "    val_accuracy = val_correct\n",
    "    print('epoch:', epoch, '  |  Train Loss:', loss, '|  Val Loss: ', val_loss, ' | Val Accuracy:', val_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869195b5-5f49-4320-862c-50e7b7e8d611",
   "metadata": {},
   "source": [
    "### 3.3 - Evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f09749a-2b54-442d-8380-e81170d619fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.999\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "\n",
    "\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == labels).sum().item()\n",
    "            total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(testdf): .3f}')\n",
    "    \n",
    "evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cdc038-6734-48f5-bc10-ece148fbe238",
   "metadata": {},
   "source": [
    "## 4 - Discussion and Conclusion:\n",
    "\n",
    "The model does outstandingly well. As pointed out in the paper, only 2 epochs are needed to train the model. The accuracy achieved of $99.9\\%$ far exceeds any of the models from the now closed Kaggle competition. One can further the results by using the full 512 charachters that the original BERT model was trained on. We can also use roBERTa, which offers a larger initial corpus to run and has pretrained libraries specificially for text classification. Both of these solutions are too large for my local machine, and consistently result in operations too large of my memory. In the Appendix, the code to train roBERTa is presented. Note that in order for the training to achive great performance, one need to tweak certain hyper parameters such as learning rates and sentence length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb740bd4-d9a4-44d9-aed0-68351bac2d19",
   "metadata": {},
   "source": [
    "## Appendix - RoBERTa:\n",
    "\n",
    "### A.1 - Introduction:\n",
    "\n",
    "RoBERTa (Robustly Optimized BERT pre-training Approach) is a language model developed by Facebook AI and introduced in 2019. It is based on the same architecture as BERT but is trained using an improved pre-training approach that enhances the original BERT model.\n",
    "RoBERTa is a bidirectional transformer-based neural network model that uses a self-attention mechanism to understand the context of the words in the input text. It consists of multiple transformer layers, each of which contains a multi-head attention mechanism, feed-forward neural network, and layer normalization. However, unlike BERT, RoBERTa is pre-trained on a larger and more diverse corpus of text data, and it uses a longer training duration and larger batch sizes. RoBERTa also employs various training techniques, such as dynamic masking and training on longer sequences, to improve its performance. Dynamic masking randomly masks out different spans of the input text during training, whereas training on longer sequences helps the model to capture longer-range dependencies between words.\n",
    "Overall, the RoBERTa architecture is highly effective in capturing the relationships between words in natural language text and has shown significant improvements in various NLP tasks, including sentiment analysis, question answering, and text classification.\n",
    "\n",
    "### A.2 - Similarities and Differences with BERT:\n",
    "\n",
    "Similarities:\n",
    "\n",
    "-Both models are based on the transformer architecture, which uses self-attention mechanisms to process input sequences.\n",
    "-Both models use a multi-layer bidirectional transformer encoder that processes the input text and a task-specific decoder that produces the final output.\n",
    "-Both models can be fine-tuned on a wide range of NLP tasks, such as sentiment analysis, question answering, and language -translation.\n",
    "\n",
    "Differences:\n",
    "\n",
    "-RoBERTa was trained on a larger and more diverse corpus of data than BERT, using additional training techniques such as dynamic masking, which helps prevent the model from memorizing specific examples during training. This has led to RoBERTa outperforming BERT on several NLP benchmarks.\n",
    "-RoBERTa removed the next sentence prediction task during training, allowing the model to better focus on the language modeling task. BERT, on the other hand, still includes this task during training.\n",
    "-RoBERTa uses larger batch sizes and longer training times than BERT, which allows it to better capture complex patterns in the data.\n",
    "-RoBERTa has shown better performance than BERT on several NLP tasks, particularly those that require more generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7277e492-185c-4c3d-a035-820a83ba6a23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2).to(device)\n",
    "\n",
    "\n",
    "train, val, test = DataClass(traindf,  tokenizer, max_length=256), DataClass(valdf, tokenizer, max_length=1024), DataClass(testdf,  tokenizer, max_length=256)\n",
    "train_dataloader = DataLoader(train, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef65a8d-28bc-4a20-8b36-5384940ffe2e",
   "metadata": {},
   "source": [
    "### A.3 - Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f51151-b99d-4b7d-bc99-57b0070de857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    accuracy = correct_predictions / len(train_dataloader.dataset)\n",
    "    # valate\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            print(outputs)\n",
    "            val_loss += outputs.loss.item()\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            val_accuracy += torch.sum(preds == labels).item()\n",
    "\n",
    "    print('epoch:', epoch, '  |  Train Loss:', train_loss, '|  Val Loss: ', val_loss, ' | Val Accuracy:', val_correct)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbabf0a9-a3c5-46a9-a5f7-b58fd9e9f385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
